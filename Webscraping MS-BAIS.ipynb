{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9163735",
   "metadata": {},
   "source": [
    "# Webscraping Information from 'https://www.usf.edu/business/graduate/ms-bais/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c484f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b35acd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following works on macos if I have gecko driver in the same folder as the script\n",
    "driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()))\n",
    "\n",
    "# load page with Selenium\n",
    "# we need to use selenium because the page loads additional records as you scroll down\n",
    "# if we used requests, we would only get the first page of speeches\n",
    "url = 'https://www.usf.edu/business/graduate/ms-bais/' \n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "#keep scrolling down until page stops loading additional records#\n",
    "pause_scroll = 4\n",
    "last_try = 0\n",
    "initialcoord = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(pause_scroll)\n",
    "    newcoord = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if newcoord == initialcoord:\n",
    "        break\n",
    "    initialcoord = newcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01b8be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.usf.edu'\n",
    "\n",
    "relative_links = [\n",
    "    \"/business/graduate/ms-info-session.aspx\",\n",
    "    \"/business/graduate/ms-bais/application-process.aspx\",\n",
    "    \"/business/graduate/ms-bais-global/index.aspx\",\n",
    "    \"/business/graduate/ms-bais/faculty.aspx\",\n",
    "    \"/business/graduate/ms-bais/\",\n",
    "    \"/business/graduate/ms-bais/faq.aspx\",\n",
    "    \"/business/graduate/ms-bais/student-spotlights.aspx\",\n",
    "    \"/business/graduate/ms-bais/new-student.aspx\"\n",
    "]\n",
    "\n",
    "link_list = [base_url + link for link in relative_links]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "314109c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_wait = 3\n",
    "\n",
    "# Lists to store scraped data\n",
    "page_urls = []\n",
    "content = []\n",
    "titles = []\n",
    "\n",
    "for link in link_list:\n",
    "    # Access page with Selenium and load html source into Beautifulsoup\n",
    "    driver.get(link)\n",
    "    time.sleep(page_wait)  # Wait for the page to load properly\n",
    "    page_source = driver.page_source\n",
    "    bsobject_msbais = bs(page_source, 'lxml')\n",
    "\n",
    "    # Add current URL to the page_urls list\n",
    "    page_urls.append(link)\n",
    "\n",
    "    # Scrape titles\n",
    "    try:\n",
    "        title = bsobject_msbais.find('h3', class_=\"mainContent_well u-flexItem--largeExtra\").text\n",
    "        titles.append(title.rstrip())\n",
    "    except AttributeError:\n",
    "        titles.append(\"No title available\")\n",
    "\n",
    "    # Scrape content\n",
    "    main_content = bsobject_msbais.find('div', class_=\"mainContent_well u-flexItem--largeExtra\")\n",
    "    if main_content:\n",
    "        paragraphs = [p.text for p in main_content.find_all('p')]\n",
    "        lists = [li.text for li in main_content.find_all(['li', 'ol', 'ul'])]\n",
    "        content.append('\\n'.join(paragraphs + lists))\n",
    "    else:\n",
    "        content.append(\"No content available\")\n",
    "\n",
    "# Ensure that you properly close the driver at the end\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5113dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Combine data into a list of dictionaries\n",
    "data = [{\"url\": link, \"title\": title, \"content\": cont} for link, title, cont in zip(link_list, titles, content)]\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"scraped_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b8896",
   "metadata": {},
   "source": [
    "# Making the json file more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d50e2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def make_content_readable(content_str):\n",
    "    \"\"\"Converts the single-string content into a list of paragraphs.\"\"\"\n",
    "    return [paragraph.strip() for paragraph in content_str.split(\"\\n\") if paragraph.strip()]\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"scraped_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify each item in the JSON data\n",
    "for item in data:\n",
    "    item['content'] = make_content_readable(item['content'])\n",
    "\n",
    "# Save the modified data to a new JSON file\n",
    "with open(\"readable_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
